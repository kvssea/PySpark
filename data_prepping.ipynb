{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATACAMP COURSE \"CLEANING DATA WITH PYSPARK\"\n",
    "# UNIT 1\n",
    "\n",
    "# Defining a Schema\n",
    "# -- useful to filter out garbage records from a dataset when reading in. \n",
    "# -- defines the datatypes of given columns, their datatype, and whether or not values can be NULL\n",
    "\n",
    "# define imports\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Define a new schema units the StructType method (note the TYPE)\n",
    "people_schema= StructType([\n",
    "    #Define a StructField for each field (note the FIELD)\n",
    "    StructField('name', StringType(), True),\n",
    "    StructField('age', IntegerType(), True),\n",
    "    StructField('city', StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Immutability andlazy processing\n",
    "# Spark is designed to use immutable objects\n",
    "# Spark is predominently functional prograaming\n",
    "# Spark dataframes are defined once and not modifiable\n",
    "# If updates are made, a new copy is made, allows spark to avoid issues with concurrent objects\n",
    "\n",
    "# read in a hypothetical dataset:\n",
    "voter_df = spark.read.csv('voterdata.csv')\n",
    "\n",
    "#making changes. THis generate a new copy with the transformatoin, and assigns it so the variable 'voter_df', eliminating the original dataframe\n",
    "voter_df = voter_df.withColumn('fullyear', voter_df.year + 2000)\n",
    "voter_df = voter_df.drop(voter_df.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lazy processing\n",
    "# spark is efficient because of lazy processing (little occurs until an action is performed)\n",
    "# In the case above, the datasets were not actually read, added or modified. We only updated the instructions until an action is called. \n",
    "# This then illustrates the point that spark has to operations, transformation (lazy) and actions (eager)\n",
    "\n",
    "voter_df.count() # here, count is considered by spark as an action, and then the action plan of all transformations prior would be executed to obtain this result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more lazy processing\n",
    "sample_df=spark.read.format('csv').option(Header=True).load('dummy_csv.csv') # lazy operation\n",
    "sample_df=sample_df.withColumn('new_col_name', F.lower(sample_df['original_col_name'])) # lazy operation\n",
    "sample_df=sample_df.drop(sample_df['original_col_name']) # lazy operation\n",
    "sample_df.show() # active operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding parquet\n",
    "\n",
    "# difficulties with CSVs - no defined schema, nested data requires special handling, limited encoding formats\n",
    "# spark has problems processing CSV data (slow to parse, can't be shared between workers at import, all data must be read before a schema can be infered if none is defined)\n",
    "# spark has 'predicate pushdown' (idea is to order tasks to do the least amount of work, something like filtering data prior to processing is one way to optimize and use 'predicate pushdown')\n",
    "# CSVs cannot be filtered via predicate pushdown\n",
    "# Spark processes are multi step and may use an intermediate file representations.\n",
    "\n",
    "# parquet files - compressed columnar format\n",
    "# structured in chunks with read/write operations without processing the entire file.\n",
    "# this format supports predicte pushdown for performance improvement\n",
    "# parquet files contain schema information\n",
    "\n",
    "# working with parquet\n",
    "# reading parquet:\n",
    "df = spark.read.format('parquet').load('filename.parquet') # ong bersion has acess to more flags (i.e. ok to overwite existing file, etc)\n",
    "df = spark.read.parquet('filename.parquet') # same but shortcut\n",
    "\n",
    "# writing\n",
    "df.write.format('parquet').save('filename.parquet') # long version has access to more flags (i.e. ok to overwrite file, etc)\n",
    "df.write.parquet('filename.parquet') # same but shortcut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet and SQL\n",
    "# parquet also are perfect for SQL Operations\n",
    "flight_df=spark.read.parquet('filename.parquet')\n",
    "flight_df.createOrReplaceTempView('flights') # assign a name to the temporary table view\n",
    "short_flights_df=spark.sql(\"SELECT * FROM flights WHERE flightduration < 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# practice with SQL in spark\n",
    "flights_df=spark.read.parquet('dummy_df.parquet')\n",
    "flights_df.createOrReplace('flights')\n",
    "avg_duration = spark.sql('SELECT avg(flight_duration) from flights').collect()[0] # note the use of 'collect' here\n",
    "print(\"The average flight time is : %d\" % avg_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIT 2 : DATAFRAME COLUMN OPERATIONS\n",
    "# Notes:\n",
    "# DATAFRAMES ARE IMMUTABLE, made up of row and columns. use transformation operations to modify data\n",
    "df.filter(df.name.lie('M%'))\n",
    "df.select('name, position')\n",
    "\n",
    "# commin transforms:\n",
    "# filter (like where in sql)\n",
    "df.filter(df.column > 100) # or df.where(...)\n",
    "\n",
    "# select - returns requsted columns\n",
    "df.select('col_name')\n",
    "\n",
    "# withColumn # creates new column in dataframe\n",
    "df.withColumn('year', df.date.year) # first arg is the new col name, second is the command to create the column\n",
    "\n",
    "# drop\n",
    "df.drop('unused_col')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILTERING DATA\n",
    "# remove nulls, remove odd entries, split data from combined sources, negate with ~\n",
    "df.filter(df['name'].isNotNull())\n",
    "df.filter(df.date.year > 1800)\n",
    "df.where(df['_c0'].contains('VOTE'))\n",
    "df.where(~ df._c1.isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLUMN STRING TRANSFORMATIONS\n",
    "# many of these functions are contained in the pyspark.sql.functions library\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df.withColumn('col_name', F.upper('name'))\n",
    "\n",
    "# we can also create intermediary columns that are useful for processig \n",
    "df.withColumn('splits', F.split('name', '')) # returns list of words in a column called splits\n",
    "\n",
    "# casting string to itneger\n",
    "df.withColumn('year', df['_c4'].cast(IntegerType()))\n",
    "\n",
    "# possibility exists to wory with ArrayType() columns (analogous to python lists)\n",
    ".size('column') # returns leng of arrayType() column\n",
    ".getItem('index') # used to retrieve a specific item at index of list column (takes index, and returns value at that index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voter_df.withColumn('splits', voter_df.split(voter_df['voter_name'], '\\s+'))\n",
    "\n",
    "voter_df.withColumn('first_name', voter_df.splits.getItem(0)) # returns the first element of each row's list.\n",
    "voter_df.withColumn('last_name', voter_df.splits.getItem(F.size('splits') - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conditional clauses\n",
    "# optimized built in conditionals. \n",
    "\n",
    "# .when() and .otherwise()\n",
    "\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# .when(<if condition>, <then x>)\n",
    "df.select(df.Name, df.Age, F.when(df.Age >= 18, \"Adult\")) # note that the select function can create arguments dyanimcally. here it would produce a column of 'adult' or blank with a blank column name\n",
    "\n",
    "\n",
    "# chaining multiple whens together\n",
    "df.select(df.Name, df.Age,.when(df.Age >= 18, \"Adult\").when(df.Age < 18, \"Minor\")) # this is the equivalent of an multiple if statements\n",
    "\n",
    "# contrasting is the use of the .otherwise() clause (essentially an 'else')\n",
    "df.select(df.Name, df.Age, .when(df.Age >= 18, \"Adult\").otherwise(\"Minor\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user defined functions (UDF)\n",
    "\n",
    "# once the udf is written, it is called using the pyspark.sql.functions.udf method\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# reverse string udf:\n",
    "# first define a python function \n",
    "def reverseString(mystr):\n",
    "    return mystr[::-1]\n",
    "\n",
    "# store function as variable for later use\n",
    "# args = the name of the method you defined, and the spark datatype that you will return from the method.\n",
    "udfReverseString=udf(reverseString, StringType())\n",
    "\n",
    "# then touse with spark, add a column to the df, passing the function variable name as the second argument\n",
    "user_df=user_df.withColumn('ReverseName', udfReverseString(user_df.Name))\n",
    "\n",
    "\n",
    "# UDFs thatdo not require arguments\n",
    "def sortingCap():\n",
    "    return random.choice(['G', 'H', 'R', 'S'])\n",
    "\n",
    "udfSortingCap = udf(sortingCap, StringType())\n",
    "user_df=user_df.withColumn(\"Class\", udfSortingCap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioning and lazy processing:\n",
    "\n",
    " # dataframes are broken up into partitions\n",
    "# partition size can vary and be optimizied\n",
    "# for now consider each paition runs independelty\n",
    "\n",
    "# transformations are LAZY (more like recipe than a command) .withColumn .select\n",
    "# nothing is actually done until an action is performed .count(), .write()\n",
    "\n",
    "# transformations can be re-ordered for the best performance\n",
    "\n",
    "\n",
    "# addind IDs (serializing number) are not very parallel so use 'monotonically_increasing_id()' incaressd value and isunique. \n",
    "# these ids are not necesarily sequential and can gave gaps. \n",
    "\n",
    "# sparks is parallel, each parition to allocated up to 8 biliion IDs to be assigned. the id are 64 bit number split into numbers. \n",
    "# into groups based on the spark partition. each group contains 8.4 billino ids and there are 2.1 millino groups none of which overlap. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import create_spark_postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=create_spark_postgres(server_name='gis_analysis', table_name='farmers_markets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----------+--------------------+--------------------+-----+-----------+----------+-------+--------------------+\n",
      "|   fmid|         market_name|              street|       city|              county|                  st|  zip|  longitude|  latitude|organic|          geog_point|\n",
      "+-------+--------------------+--------------------+-----------+--------------------+--------------------+-----+-----------+----------+-------+--------------------+\n",
      "|2000002|             Dig It!|                NULL|       NULL|                NULL|        Pennsylvania| NULL|       NULL|      NULL|      -|                NULL|\n",
      "|2000004|     Farm a la Carte|                NULL|       NULL|                NULL|             Georgia| NULL|       NULL|      NULL|      -|                NULL|\n",
      "|2000009|Freshest Cargo: M...|                NULL|       NULL|                NULL|          California| NULL|       NULL|      NULL|      -|                NULL|\n",
      "|2000013|Green Mountain - ...|                NULL|       NULL|                NULL|             Vermont| NULL|       NULL|      NULL|      -|                NULL|\n",
      "|2000022|    Real Food System|                NULL|       NULL|                NULL|            Maryland| NULL|       NULL|      NULL|      -|                NULL|\n",
      "|2000028|San Joaquin Mobil...|                NULL|       NULL|                NULL|          California| NULL|       NULL|      NULL|      -|                NULL|\n",
      "|2000034|Urban Oasis Farme...|                NULL|       NULL|                NULL|             Florida| NULL|       NULL|      NULL|      -|                NULL|\n",
      "|1012063| Caledonia Farmer...|                NULL|  Danville |           Caledonia|             Vermont|05828|-72.1403050|44.4110130|      Y|0101000020E610000...|\n",
      "|1011871| Stearns Homestea...|     6975 Ridge Road|     Parma |            Cuyahoga|                Ohio|44130|-81.7285969|41.3751180|      -|0101000020E610000...|\n",
      "|1009364|106 S. Main Stree...|  106 S. Main Street|   Six Mile|                NULL|      South Carolina|29682|-82.8187000|34.8042000|      -|0101000020E610000...|\n",
      "|1010691|10th Steet Commun...|10th Street and P...|     Lamar |              Barton|            Missouri|64759|-94.2746191|37.4956280|      -|0101000020E610000...|\n",
      "|1002454|112st Madison Avenue|112th Madison Avenue|   New York|            New York|            New York|10029|-73.9493000|40.7939000|      -|0101000020E610000...|\n",
      "|1011100|12 South Farmers ...|3000 Granny White...|  Nashville|            Davidson|           Tennessee|37204|-86.7907090|36.1183700|      Y|0101000020E610000...|\n",
      "|1009845|125th Street Fres...|163 West 125th St...|   New York|            New York|            New York|10027|-73.9482477|40.8089533|      Y|0101000020E610000...|\n",
      "|1005586|12th & Brandywine...|12th & Brandywine...| Wilmington|          New Castle|            Delaware|19801|-75.5344600|39.7421170|      N|0101000020E610000...|\n",
      "|1008071|14&U Farmers' Market|   1400 U Street NW | Washington|District of Columbia|District of Columbia|20009|-77.0320505|38.9169984|      Y|0101000020E610000...|\n",
      "|1012710|14th & Kennedy St...|5500 Colorado Ave...|Washington |District of Columbia|District of Columbia|20011|-77.0334486|38.9559783|      N|0101000020E610000...|\n",
      "|1016782|175th Street Gree...|175th Street betw...|  New York |            New York|            New York|10033|-73.9380490|40.8463540|      N|0101000020E610000...|\n",
      "|1003877|     17th Ave Market|      1622 6th St NE|Minneapolis|            Hennepin|           Minnesota|55413|-93.2591000|45.0044000|      -|0101000020E610000...|\n",
      "|1016784|17th Street Farme...|  100 North 17th St.|  Richmond |       Richmond City|            Virginia|23219|-77.4283640|37.5338500|      -|0101000020E610000...|\n",
      "+-------+--------------------+--------------------+-----------+--------------------+--------------------+-----+-----------+----------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pysp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
